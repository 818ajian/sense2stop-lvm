{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import os\n",
    "os.getcwd()\n",
    "dir = \"/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data/\"\n",
    "os.chdir(dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['8to9', '9to10', '10to11', '11to12','12to13','13to14','14to15','15to16','16to17','17to18','18to19','19to20']\n",
    "\n",
    "ec_accptresponse = ['15 to 30 minutes', '5 to 15 minutes', 'Less than 5 minutes']\n",
    "eventcontingent_dictionary = {'Less than 5 minutes': 2.5, \n",
    "                              '15 to 30 minutes': 17.5, \n",
    "                              '5 to 15 minutes': 10\n",
    "                             } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "ec_original_cloud_ids = [201, 221, 229] \n",
    "eod_original_cloud_ids = [201, 203, 206, 221, 229] \n",
    "\n",
    "contingent_ema = pd.read_csv(dir + 'eventcontingent-ema.csv')\n",
    "contingent_ema = contingent_ema.drop(['offset'], axis = 1)\n",
    "eod_ema = pd.read_csv(dir + 'eod-ema.csv')\n",
    "eod_ema = eod_ema.drop(['offset'], axis = 1)\n",
    "\n",
    "contingent_ema_alternative = pd.read_csv(dir + 'eventcontingent-ema-alternative.csv')\n",
    "eod_ema_alternative = pd.read_csv(dir + 'eod-ema-alternative.csv')\n",
    "\n",
    "contingent_ema_backup = pd.read_csv(dir + 'eventcontingent-ema-backup.csv')\n",
    "eod_ema_backup = pd.read_csv(dir + 'eod-ema-backup.csv')\n",
    "\n",
    "temp_contingent_original = contingent_ema[contingent_ema['participant_id'].isin(ec_original_cloud_ids)]\n",
    "temp_contingent_alt = contingent_ema_alternative[~contingent_ema_alternative['participant_id'].isin(ec_original_cloud_ids)]\n",
    "\n",
    "contingent_complete = pd.concat([temp_contingent_original, temp_contingent_alt, contingent_ema_backup])\n",
    "\n",
    "temp_eod_original = eod_ema[eod_ema['participant_id'].isin(eod_original_cloud_ids)]\n",
    "temp_eod_alt = eod_ema_alternative[~eod_ema_alternative['participant_id'].isin(eod_original_cloud_ids)]\n",
    "\n",
    "eod_complete = pd.concat([temp_eod_original, temp_eod_alt, eod_ema_backup])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{201: {(2017, 6, 23, 18), (2017, 6, 24, 15), (2017, 6, 24, 19)},\n",
       " 202: {(2017, 6, 26, 13),\n",
       "  (2017, 6, 27, 9),\n",
       "  (2017, 6, 27, 12),\n",
       "  (2017, 7, 6, 15)},\n",
       " 204: {(2017, 8, 14, 16),\n",
       "  (2017, 8, 14, 19),\n",
       "  (2017, 8, 15, 9),\n",
       "  (2017, 8, 15, 14),\n",
       "  (2017, 8, 15, 16)},\n",
       " 205: {(2017, 8, 18, 16),\n",
       "  (2017, 8, 18, 19),\n",
       "  (2017, 8, 19, 10),\n",
       "  (2017, 8, 19, 15),\n",
       "  (2017, 8, 20, 11),\n",
       "  (2017, 8, 20, 16)},\n",
       " 207: {(2017, 9, 18, 13),\n",
       "  (2017, 9, 18, 18),\n",
       "  (2017, 9, 19, 14),\n",
       "  (2017, 9, 19, 17),\n",
       "  (2017, 9, 20, 15),\n",
       "  (2017, 9, 22, 9)},\n",
       " 208: {(2017, 9, 18, 16),\n",
       "  (2017, 9, 19, 9),\n",
       "  (2017, 9, 19, 12),\n",
       "  (2017, 9, 19, 14),\n",
       "  (2017, 9, 20, 9),\n",
       "  (2017, 9, 20, 14),\n",
       "  (2017, 9, 20, 17),\n",
       "  (2017, 9, 24, 19),\n",
       "  (2017, 9, 26, 18),\n",
       "  (2017, 9, 27, 16),\n",
       "  (2017, 9, 28, 9),\n",
       "  (2017, 9, 28, 14)},\n",
       " 209: {(2017, 9, 29, 14),\n",
       "  (2017, 9, 29, 16),\n",
       "  (2017, 9, 29, 19),\n",
       "  (2017, 9, 30, 12)},\n",
       " 211: {(2017, 10, 2, 16),\n",
       "  (2017, 10, 2, 19),\n",
       "  (2017, 10, 3, 11),\n",
       "  (2017, 10, 3, 19),\n",
       "  (2017, 10, 4, 10),\n",
       "  (2017, 10, 4, 19),\n",
       "  (2017, 10, 7, 12)},\n",
       " 212: {(2017, 10, 13, 15), (2017, 10, 15, 10)},\n",
       " 213: {(2017, 10, 16, 16),\n",
       "  (2017, 10, 16, 19),\n",
       "  (2017, 10, 17, 15),\n",
       "  (2017, 10, 17, 18),\n",
       "  (2017, 10, 18, 13),\n",
       "  (2017, 10, 18, 15),\n",
       "  (2017, 10, 18, 18)},\n",
       " 214: {(2017, 10, 23, 13),\n",
       "  (2017, 10, 23, 17),\n",
       "  (2017, 10, 24, 9),\n",
       "  (2017, 10, 24, 13),\n",
       "  (2017, 10, 24, 18),\n",
       "  (2017, 10, 25, 14),\n",
       "  (2017, 10, 25, 16),\n",
       "  (2017, 10, 28, 18)},\n",
       " 215: {(2017, 10, 27, 14),\n",
       "  (2017, 10, 27, 17),\n",
       "  (2017, 10, 28, 16),\n",
       "  (2017, 10, 31, 17),\n",
       "  (2017, 11, 9, 13)},\n",
       " 216: {(2017, 10, 27, 17), (2017, 11, 8, 19)},\n",
       " 217: {(2017, 10, 31, 11),\n",
       "  (2017, 10, 31, 16),\n",
       "  (2017, 10, 31, 19),\n",
       "  (2017, 11, 1, 12),\n",
       "  (2017, 11, 1, 13),\n",
       "  (2017, 11, 1, 18),\n",
       "  (2017, 11, 2, 13),\n",
       "  (2017, 11, 5, 16)},\n",
       " 218: {(2017, 11, 4, 11),\n",
       "  (2017, 11, 5, 14),\n",
       "  (2017, 11, 5, 17),\n",
       "  (2017, 11, 6, 8)},\n",
       " 219: {(2017, 11, 13, 15),\n",
       "  (2017, 11, 13, 18),\n",
       "  (2017, 11, 14, 9),\n",
       "  (2017, 11, 14, 13),\n",
       "  (2017, 11, 14, 18),\n",
       "  (2017, 11, 15, 13),\n",
       "  (2017, 11, 16, 11),\n",
       "  (2017, 11, 17, 19),\n",
       "  (2017, 11, 18, 18),\n",
       "  (2017, 11, 19, 11),\n",
       "  (2017, 11, 20, 16),\n",
       "  (2017, 11, 21, 16),\n",
       "  (2017, 11, 22, 11),\n",
       "  (2017, 11, 22, 17),\n",
       "  (2017, 11, 23, 18),\n",
       "  (2017, 11, 25, 15),\n",
       "  (2017, 11, 25, 17),\n",
       "  (2017, 11, 26, 11),\n",
       "  (2017, 11, 26, 15),\n",
       "  (2017, 11, 26, 19)},\n",
       " 220: {(2017, 11, 27, 13),\n",
       "  (2017, 11, 27, 16),\n",
       "  (2017, 11, 27, 19),\n",
       "  (2017, 11, 28, 10),\n",
       "  (2017, 11, 28, 13),\n",
       "  (2017, 11, 28, 17),\n",
       "  (2017, 11, 29, 9),\n",
       "  (2017, 11, 29, 15),\n",
       "  (2017, 11, 29, 18),\n",
       "  (2017, 11, 30, 9)},\n",
       " 221: {(2017, 11, 27, 16), (2017, 11, 28, 19)},\n",
       " 222: {(2017, 12, 4, 17),\n",
       "  (2017, 12, 4, 19),\n",
       "  (2017, 12, 5, 11),\n",
       "  (2017, 12, 5, 16),\n",
       "  (2017, 12, 5, 18),\n",
       "  (2017, 12, 6, 11),\n",
       "  (2017, 12, 6, 16)},\n",
       " 223: {(2018, 1, 5, 17), (2018, 1, 6, 10), (2018, 1, 6, 14), (2018, 1, 6, 18)},\n",
       " 224: {(2018, 1, 22, 14),\n",
       "  (2018, 1, 23, 12),\n",
       "  (2018, 1, 24, 10),\n",
       "  (2018, 1, 24, 14)},\n",
       " 226: {(2018, 2, 9, 19),\n",
       "  (2018, 2, 10, 9),\n",
       "  (2018, 2, 10, 13),\n",
       "  (2018, 2, 10, 19),\n",
       "  (2018, 2, 11, 12),\n",
       "  (2018, 2, 11, 17),\n",
       "  (2018, 2, 12, 14),\n",
       "  (2018, 2, 14, 11),\n",
       "  (2018, 2, 14, 15),\n",
       "  (2018, 2, 16, 10),\n",
       "  (2018, 2, 17, 12)},\n",
       " 227: {(2018, 2, 10, 9),\n",
       "  (2018, 2, 10, 13),\n",
       "  (2018, 2, 10, 17),\n",
       "  (2018, 2, 12, 16),\n",
       "  (2018, 2, 12, 19),\n",
       "  (2018, 2, 13, 11),\n",
       "  (2018, 2, 13, 15),\n",
       "  (2018, 2, 13, 19),\n",
       "  (2018, 2, 14, 9),\n",
       "  (2018, 2, 14, 15),\n",
       "  (2018, 2, 14, 18),\n",
       "  (2018, 2, 15, 10),\n",
       "  (2018, 2, 15, 15),\n",
       "  (2018, 2, 15, 16),\n",
       "  (2018, 2, 15, 17),\n",
       "  (2018, 2, 15, 18),\n",
       "  (2018, 2, 16, 10),\n",
       "  (2018, 2, 16, 13),\n",
       "  (2018, 2, 16, 19),\n",
       "  (2018, 2, 17, 9),\n",
       "  (2018, 2, 17, 13),\n",
       "  (2018, 2, 17, 17),\n",
       "  (2018, 2, 18, 10),\n",
       "  (2018, 2, 18, 14),\n",
       "  (2018, 2, 18, 18),\n",
       "  (2018, 2, 19, 10),\n",
       "  (2018, 2, 19, 13),\n",
       "  (2018, 2, 19, 16),\n",
       "  (2018, 2, 20, 9),\n",
       "  (2018, 2, 20, 13),\n",
       "  (2018, 2, 20, 17),\n",
       "  (2018, 2, 21, 10),\n",
       "  (2018, 2, 21, 14),\n",
       "  (2018, 2, 21, 19),\n",
       "  (2018, 2, 22, 10),\n",
       "  (2018, 2, 22, 15),\n",
       "  (2018, 2, 22, 18),\n",
       "  (2018, 2, 23, 8)},\n",
       " 228: {(2018, 4, 6, 14),\n",
       "  (2018, 4, 6, 19),\n",
       "  (2018, 4, 7, 10),\n",
       "  (2018, 4, 7, 13),\n",
       "  (2018, 4, 7, 19),\n",
       "  (2018, 4, 8, 11),\n",
       "  (2018, 4, 8, 12),\n",
       "  (2018, 4, 8, 19),\n",
       "  (2018, 4, 11, 15),\n",
       "  (2018, 4, 16, 9),\n",
       "  (2018, 4, 17, 9),\n",
       "  (2018, 4, 18, 9),\n",
       "  (2018, 4, 18, 12),\n",
       "  (2018, 4, 18, 14),\n",
       "  (2018, 4, 19, 9),\n",
       "  (2018, 4, 19, 14),\n",
       "  (2018, 4, 20, 9)},\n",
       " 229: {(2018, 4, 13, 14),\n",
       "  (2018, 4, 13, 18),\n",
       "  (2018, 4, 14, 14),\n",
       "  (2018, 4, 14, 16),\n",
       "  (2018, 4, 15, 9),\n",
       "  (2018, 4, 15, 12),\n",
       "  (2018, 4, 15, 16),\n",
       "  (2018, 4, 16, 10),\n",
       "  (2018, 4, 18, 10),\n",
       "  (2018, 4, 19, 9)},\n",
       " 230: {(2018, 6, 18, 15),\n",
       "  (2018, 6, 20, 8),\n",
       "  (2018, 6, 20, 12),\n",
       "  (2018, 6, 20, 15)},\n",
       " 231: {(2018, 6, 18, 15),\n",
       "  (2018, 6, 19, 10),\n",
       "  (2018, 6, 19, 14),\n",
       "  (2018, 6, 19, 17),\n",
       "  (2018, 6, 20, 8),\n",
       "  (2018, 6, 20, 11)},\n",
       " 233: {(2018, 7, 13, 13),\n",
       "  (2018, 7, 13, 19),\n",
       "  (2018, 7, 14, 11),\n",
       "  (2018, 7, 14, 17),\n",
       "  (2018, 7, 15, 12),\n",
       "  (2018, 7, 18, 19),\n",
       "  (2018, 7, 22, 19)},\n",
       " 234: {(2018, 7, 16, 17),\n",
       "  (2018, 7, 17, 12),\n",
       "  (2018, 7, 17, 15),\n",
       "  (2018, 7, 18, 11)},\n",
       " 235: {(2018, 7, 20, 13),\n",
       "  (2018, 7, 20, 18),\n",
       "  (2018, 7, 21, 12),\n",
       "  (2018, 7, 22, 12),\n",
       "  (2018, 7, 22, 15)},\n",
       " 237: {(2018, 7, 27, 13)}}"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of all contingent event-times between 8AM and 8PM\n",
    "# Throw away observations for 'when_smoke' is nan or \n",
    "# 'More than 30 minutes' to ensure we can calculate a meaningful \n",
    "# quantity.\n",
    "days_smoked = {}\n",
    "for index, row in contingent_complete.iterrows():\n",
    "    count += 1\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    if row['when_smoke'] in ec_accptresponse:\n",
    "        time = time - datetime.timedelta(minutes=eventcontingent_dictionary[row['when_smoke']])\n",
    "    date = (time.year, time.month, time.day, time.hour)\n",
    "    if row['participant_id'] not in days_smoked:\n",
    "        days_smoked[row['participant_id']] = set()\n",
    "    if 8 <= date[3] < 20 and row['when_smoke'] in ec_accptresponse:        \n",
    "        days_smoked[row['participant_id']].add(date)\n",
    "\n",
    "days_smoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [203, 2017, 8, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [236, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a list of id + dates for eod\n",
    "# Use to look \n",
    "eod_dates = []\n",
    "for irow in range(0,eod_complete.shape[0]):\n",
    "    row = eod_complete.iloc[irow]\n",
    "    if row['status'] == \"MISSED\":\n",
    "        continue\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%Y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    if time.hour  == 0 or time.hour == 1:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day-1])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    else:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    eod_dates.append(date)\n",
    "    \n",
    "eod_dates = np.asarray(eod_dates)\n",
    "eod_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current hour only:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.544\n",
      "Mean of Fraction agreement across indidivuals: 0.467\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.322\n",
      "\n",
      "Plus-minus one hour:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.841\n",
      "Mean of Fraction agreement across indidivuals: 0.745\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.34\n"
     ]
    }
   ],
   "source": [
    "# For participants with both EC and EOD measurements,\n",
    "# on days when you give both, we ask whether they agree,\n",
    "# up to the current hour, or +- 1 hour in either direction.\n",
    "# The +-1 is max/min by 8AM and 8PM respectively.\n",
    "matching_counts = []\n",
    "max_iloc = 15; min_iloc = 4\n",
    "for id in set(days_smoked.keys()) & set(eod_dates[:,0]):\n",
    "    eod_dates_id = np.where(eod_dates[:,0] == id) \n",
    "    eod_dates_subset = eod_dates[eod_dates_id[0],:]\n",
    "    total_count_id = 0\n",
    "    hour_count_id_true = 0\n",
    "    twohour_count_id_true = 0\n",
    "    for ec_time in days_smoked[id]:\n",
    "        row_iloc = np.where((eod_dates_subset[:,1:4] == ec_time[0:3]).all(axis=1))[0]\n",
    "        if not row_iloc.size > 0:\n",
    "            continue\n",
    "        total_count_id+=1\n",
    "        row = eod_dates_subset[row_iloc][0]\n",
    "        ec_iloc = range(8,20).index(ec_time[3])+4\n",
    "        if row[ec_iloc]==1:\n",
    "            hour_count_id_true+=1\n",
    "        if any(row[range(max(min_iloc, ec_iloc-1), min(max_iloc, ec_iloc+1)+1)] == 1):\n",
    "            twohour_count_id_true+=1\n",
    "    matching_counts.append(np.array([total_count_id, hour_count_id_true, twohour_count_id_true], dtype='f'))\n",
    "\n",
    "matching_counts = np.asarray(matching_counts)\n",
    "\n",
    "matching_counts = np.delete(matching_counts, (np.where(matching_counts[:,0] == 0)[0][0]), axis=0)\n",
    "\n",
    "fraction_per_id_onehour = np.divide(matching_counts[:,1],matching_counts[:,0])\n",
    "fraction_per_id_twohour = np.divide(matching_counts[:,2],matching_counts[:,0])\n",
    "\n",
    "aggregate_matching_counts = np.sum(matching_counts, axis=0)\n",
    "\n",
    "aggregate_frac_onehour = aggregate_matching_counts[1]/aggregate_matching_counts[0]\n",
    "aggregate_frac_twohour = aggregate_matching_counts[2]/aggregate_matching_counts[0]\n",
    "\n",
    "print 'Current hour only:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_onehour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_onehour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_onehour),3))\n",
    "print\n",
    "print 'Plus-minus one hour:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_twohour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_twohour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_twohour),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA p-value for current hour: 2.422465948948016e-06\n",
      "ANOVA p-value for plus-minus one hour: 8.337050984019712e-07\n"
     ]
    }
   ],
   "source": [
    "# Compute an anova decomposition using the bernoulli likelihood\n",
    "# This will test if there are significant differences across\n",
    "# individuals.\n",
    "\n",
    "llik_onehour = 0; llik_twohour = 0\n",
    "for i in range(0, fraction_per_id_onehour.size):\n",
    "    num_ones_onehour = matching_counts[i,1]\n",
    "    num_zeros_onehour = matching_counts[i,0] - matching_counts[i,1]\n",
    "    if num_ones_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_ones_onehour, np.log(fraction_per_id_onehour[i]))\n",
    "    if num_zeros_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_zeros_onehour, np.log(1-fraction_per_id_onehour[i]))\n",
    "    num_ones_twohour = matching_counts[i,2]\n",
    "    num_zeros_twohour = matching_counts[i,0] - matching_counts[i,2]\n",
    "    if num_ones_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_ones_twohour, np.log(fraction_per_id_twohour[i]))\n",
    "    if num_zeros_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_zeros_twohour, np.log(1-fraction_per_id_twohour[i]))\n",
    "\n",
    "agg_num_ones = aggregate_matching_counts[1]\n",
    "agg_num_zeros = aggregate_matching_counts[0] - aggregate_matching_counts[1]\n",
    "agg_llik_onehour = agg_num_ones*np.log(aggregate_frac_onehour)+agg_num_zeros*np.log(1-aggregate_frac_onehour)\n",
    "\n",
    "D_onehour = -2*agg_llik_onehour + 2*llik_onehour\n",
    "\n",
    "agg_num_ones_twohour = aggregate_matching_counts[2]\n",
    "agg_num_zeros_twohour = aggregate_matching_counts[0] - aggregate_matching_counts[2]\n",
    "agg_llik_twohour = agg_num_ones_twohour*np.log(aggregate_frac_twohour)+agg_num_zeros_twohour*np.log(1-aggregate_frac_twohour)\n",
    "\n",
    "D_twohour = -2*agg_llik_twohour + 2*llik_twohour\n",
    "\n",
    "from scipy.stats import chi2\n",
    "n = aggregate_matching_counts[0]\n",
    "k = matching_counts.shape[0]\n",
    "df = k-1\n",
    "\n",
    "print 'ANOVA p-value for current hour: %s' % (1-chi2.cdf(D_onehour, df))\n",
    "print 'ANOVA p-value for plus-minus one hour: %s' % (1-chi2.cdf(D_twohour, df))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
