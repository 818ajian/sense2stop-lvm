{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import os\n",
    "os.getcwd()\n",
    "dir = \"/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data/\"\n",
    "os.chdir(dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['8to9', '9to10', '10to11', '11to12','12to13','13to14','14to15','15to16','16to17','17to18','18to19','19to20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'timestamp', u'event', u'participant_id', u'date', u'hour', u'minute',\n",
       "       u'day_of_week'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "episode_original_cloud_ids = [201, 203, 206, 210, 229] \n",
    "eod_original_cloud_ids = [201, 203, 206, 221, 229] \n",
    "\n",
    "puff_episode = pd.read_csv(dir + 'puff-episode.csv')\n",
    "puff_episode = puff_episode.drop(['offset'], axis = 1)\n",
    "eod_ema = pd.read_csv(dir + 'eod-ema.csv')\n",
    "eod_ema = eod_ema.drop(['offset'], axis = 1)\n",
    "\n",
    "puff_episode_alt = pd.read_csv('puff-episode-alternative.csv')\n",
    "eod_ema_alternative = pd.read_csv(dir + 'eod-ema-alternative.csv')\n",
    "\n",
    "puff_episode_backup = pd.read_csv('puff-episode-backup.csv')\n",
    "eod_ema_backup = pd.read_csv(dir + 'eod-ema-backup.csv')\n",
    "\n",
    "temp_puff_episode = puff_episode[puff_episode['participant_id'].isin(episode_original_cloud_ids)]\n",
    "temp_puff_episode_alt = puff_episode_alt[~puff_episode_alt['participant_id'].isin(episode_original_cloud_ids)]\n",
    "\n",
    "puff_complete = pd.concat([temp_puff_episode, temp_puff_episode_alt, puff_episode_backup])\n",
    "\n",
    "temp_eod_original = eod_ema[eod_ema['participant_id'].isin(eod_original_cloud_ids)]\n",
    "temp_eod_alt = eod_ema_alternative[~eod_ema_alternative['participant_id'].isin(eod_original_cloud_ids)]\n",
    "\n",
    "eod_complete = pd.concat([temp_eod_original, temp_eod_alt, eod_ema_backup])\n",
    "\n",
    "puff_complete.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{201.0: {(2017, 6, 24, 17)},\n",
       " 202.0: {(2017, 6, 27, 8)},\n",
       " 203.0: set(),\n",
       " 204.0: {(2017, 8, 14, 16),\n",
       "  (2017, 8, 14, 17),\n",
       "  (2017, 8, 14, 19),\n",
       "  (2017, 8, 15, 9),\n",
       "  (2017, 8, 15, 10),\n",
       "  (2017, 8, 15, 12),\n",
       "  (2017, 8, 15, 14)},\n",
       " 205.0: {(2017, 8, 18, 16),\n",
       "  (2017, 8, 18, 17),\n",
       "  (2017, 8, 18, 19),\n",
       "  (2017, 8, 19, 9),\n",
       "  (2017, 8, 19, 10),\n",
       "  (2017, 8, 20, 11),\n",
       "  (2017, 8, 21, 11),\n",
       "  (2017, 8, 22, 14),\n",
       "  (2017, 8, 22, 15),\n",
       "  (2017, 8, 22, 16),\n",
       "  (2017, 8, 22, 17),\n",
       "  (2017, 8, 23, 12),\n",
       "  (2017, 8, 23, 13),\n",
       "  (2017, 8, 23, 14),\n",
       "  (2017, 8, 23, 15),\n",
       "  (2017, 8, 23, 18),\n",
       "  (2017, 8, 25, 9),\n",
       "  (2017, 8, 27, 15),\n",
       "  (2017, 8, 28, 9),\n",
       "  (2017, 8, 28, 12),\n",
       "  (2017, 8, 28, 13),\n",
       "  (2017, 8, 28, 14),\n",
       "  (2017, 8, 28, 17),\n",
       "  (2017, 8, 30, 10),\n",
       "  (2017, 8, 30, 18)},\n",
       " 206.0: {(2017, 9, 25, 9)},\n",
       " 207.0: {(2017, 9, 22, 15),\n",
       "  (2017, 9, 23, 16),\n",
       "  (2017, 9, 25, 11),\n",
       "  (2017, 9, 25, 18),\n",
       "  (2017, 9, 26, 13),\n",
       "  (2017, 9, 27, 11),\n",
       "  (2017, 9, 27, 12),\n",
       "  (2017, 9, 27, 13),\n",
       "  (2017, 9, 27, 14),\n",
       "  (2017, 9, 27, 16),\n",
       "  (2017, 9, 28, 11),\n",
       "  (2017, 9, 29, 11)},\n",
       " 208.0: {(2017, 9, 18, 15),\n",
       "  (2017, 9, 18, 16),\n",
       "  (2017, 9, 20, 18),\n",
       "  (2017, 9, 27, 16),\n",
       "  (2017, 9, 27, 17)},\n",
       " 209.0: {(2017, 9, 30, 15)},\n",
       " 210.0: {(2017, 9, 29, 14)},\n",
       " 211.0: {(2017, 10, 2, 19),\n",
       "  (2017, 10, 3, 9),\n",
       "  (2017, 10, 3, 11),\n",
       "  (2017, 10, 4, 10),\n",
       "  (2017, 10, 4, 13),\n",
       "  (2017, 10, 4, 17),\n",
       "  (2017, 10, 5, 14),\n",
       "  (2017, 10, 7, 12),\n",
       "  (2017, 10, 9, 17),\n",
       "  (2017, 10, 12, 13),\n",
       "  (2017, 10, 12, 17),\n",
       "  (2017, 10, 12, 18),\n",
       "  (2017, 10, 13, 17),\n",
       "  (2017, 10, 14, 13),\n",
       "  (2017, 10, 14, 19)},\n",
       " 212.0: {(2017, 10, 13, 16)},\n",
       " 213.0: {(2017, 10, 16, 17),\n",
       "  (2017, 10, 17, 13),\n",
       "  (2017, 10, 17, 16),\n",
       "  (2017, 10, 17, 17),\n",
       "  (2017, 10, 17, 18),\n",
       "  (2017, 10, 18, 11),\n",
       "  (2017, 10, 18, 15)},\n",
       " 214.0: {(2017, 10, 23, 14),\n",
       "  (2017, 10, 23, 15),\n",
       "  (2017, 10, 23, 17),\n",
       "  (2017, 10, 23, 18),\n",
       "  (2017, 10, 24, 9),\n",
       "  (2017, 10, 24, 10),\n",
       "  (2017, 10, 24, 12),\n",
       "  (2017, 10, 24, 13),\n",
       "  (2017, 10, 24, 18),\n",
       "  (2017, 10, 25, 17),\n",
       "  (2017, 10, 25, 18),\n",
       "  (2017, 10, 25, 19),\n",
       "  (2017, 10, 26, 18),\n",
       "  (2017, 10, 27, 16),\n",
       "  (2017, 10, 28, 16),\n",
       "  (2017, 10, 28, 17),\n",
       "  (2017, 10, 28, 18),\n",
       "  (2017, 10, 28, 19),\n",
       "  (2017, 10, 30, 10),\n",
       "  (2017, 10, 30, 14),\n",
       "  (2017, 10, 31, 14)},\n",
       " 215.0: {(2017, 10, 28, 16),\n",
       "  (2017, 10, 30, 12),\n",
       "  (2017, 10, 31, 17),\n",
       "  (2017, 11, 2, 12),\n",
       "  (2017, 11, 8, 9)},\n",
       " 216.0: {(2017, 10, 28, 16),\n",
       "  (2017, 10, 28, 18),\n",
       "  (2017, 11, 2, 10),\n",
       "  (2017, 11, 2, 13),\n",
       "  (2017, 11, 2, 17),\n",
       "  (2017, 11, 6, 15),\n",
       "  (2017, 11, 6, 16)},\n",
       " 217.0: {(2017, 11, 2, 9),\n",
       "  (2017, 11, 2, 12),\n",
       "  (2017, 11, 2, 14),\n",
       "  (2017, 11, 4, 13),\n",
       "  (2017, 11, 4, 14),\n",
       "  (2017, 11, 4, 16),\n",
       "  (2017, 11, 5, 14),\n",
       "  (2017, 11, 8, 18)},\n",
       " 218.0: {(2017, 11, 14, 8),\n",
       "  (2017, 11, 14, 9),\n",
       "  (2017, 11, 14, 10),\n",
       "  (2017, 11, 14, 12),\n",
       "  (2017, 11, 14, 14),\n",
       "  (2017, 11, 15, 12),\n",
       "  (2017, 11, 15, 16)},\n",
       " 219.0: {(2017, 11, 14, 13),\n",
       "  (2017, 11, 14, 17),\n",
       "  (2017, 11, 15, 19),\n",
       "  (2017, 11, 16, 16),\n",
       "  (2017, 11, 16, 17),\n",
       "  (2017, 11, 17, 15),\n",
       "  (2017, 11, 17, 19),\n",
       "  (2017, 11, 20, 16),\n",
       "  (2017, 11, 21, 16),\n",
       "  (2017, 11, 21, 19),\n",
       "  (2017, 11, 22, 14),\n",
       "  (2017, 11, 22, 15),\n",
       "  (2017, 11, 23, 18),\n",
       "  (2017, 11, 25, 19)},\n",
       " 220.0: {(2017, 11, 27, 13),\n",
       "  (2017, 11, 27, 14),\n",
       "  (2017, 11, 28, 9),\n",
       "  (2017, 11, 28, 10),\n",
       "  (2017, 11, 28, 13),\n",
       "  (2017, 11, 28, 17),\n",
       "  (2017, 11, 29, 17),\n",
       "  (2017, 11, 30, 8),\n",
       "  (2017, 11, 30, 9)},\n",
       " 222.0: {(2017, 12, 6, 10),\n",
       "  (2017, 12, 6, 12),\n",
       "  (2017, 12, 6, 15),\n",
       "  (2017, 12, 6, 19),\n",
       "  (2017, 12, 7, 9),\n",
       "  (2017, 12, 8, 16),\n",
       "  (2017, 12, 8, 19)},\n",
       " 223.0: {(2018, 1, 6, 14),\n",
       "  (2018, 1, 6, 18),\n",
       "  (2018, 1, 8, 9),\n",
       "  (2018, 1, 8, 13),\n",
       "  (2018, 1, 8, 14),\n",
       "  (2018, 1, 10, 13),\n",
       "  (2018, 1, 10, 18),\n",
       "  (2018, 1, 11, 10),\n",
       "  (2018, 1, 11, 15),\n",
       "  (2018, 1, 12, 11),\n",
       "  (2018, 1, 12, 14),\n",
       "  (2018, 1, 12, 15),\n",
       "  (2018, 1, 12, 17),\n",
       "  (2018, 1, 13, 10),\n",
       "  (2018, 1, 13, 13)},\n",
       " 224.0: {(2018, 1, 24, 8), (2018, 1, 25, 8)},\n",
       " 225.0: {(2018, 1, 29, 13),\n",
       "  (2018, 1, 30, 8),\n",
       "  (2018, 1, 30, 9),\n",
       "  (2018, 1, 30, 11),\n",
       "  (2018, 1, 30, 13),\n",
       "  (2018, 1, 30, 14),\n",
       "  (2018, 1, 31, 9),\n",
       "  (2018, 1, 31, 16),\n",
       "  (2018, 2, 4, 13)},\n",
       " 226.0: {(2018, 2, 10, 10),\n",
       "  (2018, 2, 10, 12),\n",
       "  (2018, 2, 10, 13),\n",
       "  (2018, 2, 11, 9),\n",
       "  (2018, 2, 13, 12),\n",
       "  (2018, 2, 14, 10),\n",
       "  (2018, 2, 14, 19),\n",
       "  (2018, 2, 15, 9),\n",
       "  (2018, 2, 15, 16),\n",
       "  (2018, 2, 15, 18),\n",
       "  (2018, 2, 19, 9),\n",
       "  (2018, 2, 19, 11),\n",
       "  (2018, 2, 20, 10),\n",
       "  (2018, 2, 20, 19),\n",
       "  (2018, 2, 21, 8)},\n",
       " 227.0: {(2018, 2, 17, 19)},\n",
       " 228.0: {(2018, 4, 8, 11),\n",
       "  (2018, 4, 8, 12),\n",
       "  (2018, 4, 8, 13),\n",
       "  (2018, 4, 8, 14),\n",
       "  (2018, 4, 8, 16),\n",
       "  (2018, 4, 13, 9),\n",
       "  (2018, 4, 13, 18),\n",
       "  (2018, 4, 14, 9),\n",
       "  (2018, 4, 16, 9),\n",
       "  (2018, 4, 16, 15),\n",
       "  (2018, 4, 17, 12),\n",
       "  (2018, 4, 17, 15),\n",
       "  (2018, 4, 17, 18),\n",
       "  (2018, 4, 18, 9),\n",
       "  (2018, 4, 18, 15),\n",
       "  (2018, 4, 19, 9),\n",
       "  (2018, 4, 19, 12),\n",
       "  (2018, 4, 19, 18),\n",
       "  (2018, 4, 20, 9)},\n",
       " 229.0: {(2018, 4, 13, 14),\n",
       "  (2018, 4, 13, 18),\n",
       "  (2018, 4, 14, 9),\n",
       "  (2018, 4, 14, 15),\n",
       "  (2018, 4, 14, 18),\n",
       "  (2018, 4, 15, 9),\n",
       "  (2018, 4, 15, 19),\n",
       "  (2018, 4, 16, 12),\n",
       "  (2018, 4, 16, 14)},\n",
       " 230.0: {(2018, 6, 18, 15),\n",
       "  (2018, 6, 18, 17),\n",
       "  (2018, 6, 18, 18),\n",
       "  (2018, 6, 26, 17),\n",
       "  (2018, 6, 27, 8),\n",
       "  (2018, 6, 27, 18)},\n",
       " 231.0: {(2018, 6, 19, 12),\n",
       "  (2018, 6, 19, 15),\n",
       "  (2018, 6, 19, 17),\n",
       "  (2018, 6, 19, 18)},\n",
       " 233.0: set(),\n",
       " 234.0: {(2018, 7, 17, 16), (2018, 7, 17, 17), (2018, 7, 18, 14)},\n",
       " 235.0: {(2018, 7, 20, 13), (2018, 7, 22, 16), (2018, 7, 24, 18)}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of all contingent event-times between 8AM and 8PM\n",
    "# Throw away observations for 'when_smoke' is nan or \n",
    "# 'More than 30 minutes' to ensure we can calculate a meaningful \n",
    "# quantity.\n",
    "days_smoked = {}\n",
    "for index, row in puff_complete.iterrows():\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    date = (time.year, time.month, time.day, time.hour)\n",
    "    if row['participant_id'] not in days_smoked:\n",
    "        days_smoked[row['participant_id']] = set()\n",
    "    if 8 <= date[3] < 20:        \n",
    "        days_smoked[row['participant_id']].add(date)\n",
    "\n",
    "days_smoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [203, 2017, 8, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [236, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a list of id + dates for eod\n",
    "# Use to look \n",
    "eod_dates = []\n",
    "for irow in range(0,eod_complete.shape[0]):\n",
    "    row = eod_complete.iloc[irow]\n",
    "    if row['status'] == \"MISSED\":\n",
    "        continue\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%Y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    if time.hour  == 0 or time.hour == 1:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day-1])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    else:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    eod_dates.append(date)\n",
    "    \n",
    "eod_dates = np.asarray(eod_dates)\n",
    "eod_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  1.]\n",
      " [ 7.  7.  7.]\n",
      " [25.  3.  6.]\n",
      " [ 1.  0.  0.]\n",
      " [10.  0.  0.]\n",
      " [ 5.  2.  4.]\n",
      " [ 1.  1.  1.]\n",
      " [15.  4.  7.]\n",
      " [ 7.  3.  7.]\n",
      " [21. 16. 16.]\n",
      " [ 5.  1.  2.]\n",
      " [ 7.  1.  1.]\n",
      " [ 8.  1.  8.]\n",
      " [ 7.  0.  0.]\n",
      " [14.  8. 13.]\n",
      " [ 3.  1.  3.]\n",
      " [ 7.  0.  1.]\n",
      " [15.  1.  3.]\n",
      " [ 2.  1.  1.]\n",
      " [ 7.  0.  0.]\n",
      " [12.  2. 11.]\n",
      " [ 1.  0.  0.]\n",
      " [18.  5. 11.]\n",
      " [ 7.  3.  6.]\n",
      " [ 6.  2.  2.]\n",
      " [ 4.  3.  4.]\n",
      " [ 3.  0.  2.]]\n",
      "Current hour only:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.297\n",
      "Mean of Fraction agreement across indidivuals: 0.292\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.301\n",
      "\n",
      "Plus-minus one hour:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.534\n",
      "Mean of Fraction agreement across indidivuals: 0.554\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.388\n"
     ]
    }
   ],
   "source": [
    "# For participants with both EC and EOD measurements,\n",
    "# on days when you give both, we ask whether they agree,\n",
    "# up to the current hour, or +- 1 hour in either direction.\n",
    "# The +-1 is max/min by 8AM and 8PM respectively.\n",
    "matching_counts = []\n",
    "max_iloc = 15; min_iloc = 4\n",
    "for id in set(days_smoked.keys()) & set(eod_dates[:,0]):\n",
    "    eod_dates_id = np.where(eod_dates[:,0] == id) \n",
    "    eod_dates_subset = eod_dates[eod_dates_id[0],:]\n",
    "    total_count_id = 0\n",
    "    hour_count_id_true = 0\n",
    "    twohour_count_id_true = 0\n",
    "    for ec_time in days_smoked[id]:\n",
    "        row_iloc = np.where((eod_dates_subset[:,1:4] == ec_time[0:3]).all(axis=1))[0]\n",
    "        if not row_iloc.size > 0:\n",
    "            continue\n",
    "        total_count_id+=1\n",
    "        row = eod_dates_subset[row_iloc][0]\n",
    "        ec_iloc = range(8,20).index(ec_time[3])+4\n",
    "        if row[ec_iloc]==1:\n",
    "            hour_count_id_true+=1\n",
    "        if any(row[range(max(min_iloc, ec_iloc-1), min(max_iloc, ec_iloc+1)+1)] == 1):\n",
    "            twohour_count_id_true+=1\n",
    "    matching_counts.append(np.array([total_count_id, hour_count_id_true, twohour_count_id_true], dtype='f'))\n",
    "\n",
    "matching_counts = np.asarray(matching_counts)\n",
    "\n",
    "matching_counts = np.delete(matching_counts, (np.where(matching_counts[:,0] == 0)[0]), axis=0)\n",
    "\n",
    "print matching_counts\n",
    "\n",
    "fraction_per_id_onehour = np.divide(matching_counts[:,1],matching_counts[:,0])\n",
    "fraction_per_id_twohour = np.divide(matching_counts[:,2],matching_counts[:,0])\n",
    "\n",
    "aggregate_matching_counts = np.sum(matching_counts, axis=0)\n",
    "\n",
    "aggregate_frac_onehour = aggregate_matching_counts[1]/aggregate_matching_counts[0]\n",
    "aggregate_frac_twohour = aggregate_matching_counts[2]/aggregate_matching_counts[0]\n",
    "\n",
    "print 'Current hour only:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_onehour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_onehour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_onehour),3))\n",
    "print\n",
    "print 'Plus-minus one hour:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_twohour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_twohour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_twohour),3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA p-value for current hour: 1.3191822412217391e-08\n",
      "ANOVA p-value for plus-minus one hour: 1.1102230246251565e-16\n"
     ]
    }
   ],
   "source": [
    "# Compute an anova decomposition using the bernoulli likelihood\n",
    "# This will test if there are significant differences across\n",
    "# individuals.\n",
    "\n",
    "llik_onehour = 0; llik_twohour = 0\n",
    "for i in range(0, fraction_per_id_onehour.size):\n",
    "    num_ones_onehour = matching_counts[i,1]\n",
    "    num_zeros_onehour = matching_counts[i,0] - matching_counts[i,1]\n",
    "    if num_ones_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_ones_onehour, np.log(fraction_per_id_onehour[i]))\n",
    "    if num_zeros_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_zeros_onehour, np.log(1-fraction_per_id_onehour[i]))\n",
    "    num_ones_twohour = matching_counts[i,2]\n",
    "    num_zeros_twohour = matching_counts[i,0] - matching_counts[i,2]\n",
    "    if num_ones_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_ones_twohour, np.log(fraction_per_id_twohour[i]))\n",
    "    if num_zeros_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_zeros_twohour, np.log(1-fraction_per_id_twohour[i]))\n",
    "\n",
    "agg_num_ones = aggregate_matching_counts[1]\n",
    "agg_num_zeros = aggregate_matching_counts[0] - aggregate_matching_counts[1]\n",
    "agg_llik_onehour = agg_num_ones*np.log(aggregate_frac_onehour)+agg_num_zeros*np.log(1-aggregate_frac_onehour)\n",
    "\n",
    "D_onehour = -2*agg_llik_onehour + 2*llik_onehour\n",
    "\n",
    "agg_num_ones_twohour = aggregate_matching_counts[2]\n",
    "agg_num_zeros_twohour = aggregate_matching_counts[0] - aggregate_matching_counts[2]\n",
    "agg_llik_twohour = agg_num_ones_twohour*np.log(aggregate_frac_twohour)+agg_num_zeros_twohour*np.log(1-aggregate_frac_twohour)\n",
    "\n",
    "D_twohour = -2*agg_llik_twohour + 2*llik_twohour\n",
    "\n",
    "from scipy.stats import chi2\n",
    "n = aggregate_matching_counts[0]\n",
    "k = matching_counts.shape[0]\n",
    "df = k-1\n",
    "\n",
    "print 'ANOVA p-value for current hour: %s' % (1-chi2.cdf(D_onehour, df))\n",
    "print 'ANOVA p-value for plus-minus one hour: %s' % (1-chi2.cdf(D_twohour, df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_vs_eod(puff_episode, eod_ema):\n",
    "    '''\n",
    "    checks how reliable is the eod_ema in tracking Hand-to-Mouth gestures\n",
    "    '''\n",
    "    \n",
    "    days_smoked = {}\n",
    "    \n",
    "    # store smoking events recorded in eod_ema\n",
    "    for index, row in eod_ema.iterrows():\n",
    "        if row['status'] == \"MISSED\":\n",
    "            continue\n",
    "        for i in keys:\n",
    "            if row[i] == 1:\n",
    "                try:\n",
    "                    time = datetime.datetime.strptime(str(row['date']), '%m/%d/%Y %H:%M')\n",
    "                except:\n",
    "                    time = datetime.datetime.strptime(str(row['date']), '%Y-%m-%d %H:%M:%S')\n",
    "                date = (time.year, time.month, time.day)\n",
    "                if time.hour  == 0 or time.hour == 1:\n",
    "                    date = (time.year, time.month, time.day-1)\n",
    "                if row['participant_id'] not in days_smoked:\n",
    "                    days_smoked[row['participant_id']] = set()\n",
    "                days_smoked[row['participant_id']].add(date)\n",
    "                break\n",
    "\n",
    "    count = 0 \n",
    "    missing = 0\n",
    "    missing_days = 0\n",
    "    lst = []\n",
    "    events = set()\n",
    "    \n",
    "    # store HTMGs \n",
    "    for index, row in puff_episode.iterrows():\n",
    "        try:\n",
    "            count += 1\n",
    "            try:\n",
    "                time = datetime.datetime.strptime(str(row['date']), '%m/%d/%y %H:%M')\n",
    "            except:\n",
    "                time = datetime.datetime.strptime(str(row['date']), '%Y-%m-%d %H:%M:%S')\n",
    "            date = (time.year, time.month, time.day)\n",
    "            if row['participant_id'] not in days_smoked:\n",
    "                continue\n",
    "            if date not in days_smoked[row['participant_id']]:\n",
    "                missing += 1\n",
    "                if date not in events:\n",
    "                    missing_days += 1\n",
    "                    events.add(date)\n",
    "                lst.append(index)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    missing_in_eod = puff_episode.ix[lst]\n",
    "    # percentage of HTMGs missed by eod_ema\n",
    "    print(\"Puff-Episode vs. EOD EMA inconsistency percentage by entries is: \", missing/float(count))\n",
    "    print(\"Puff-Episode vs. EOD EMA inconsistency percentage by day is: \", missing_days/float(count))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Puff-Episode vs. EOD EMA inconsistency percentage by entries is: ', 0.2981132075471698)\n",
      "('Puff-Episode vs. EOD EMA inconsistency percentage by day is: ', 0.12075471698113208)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "episode_vs_eod(puff_episode, eod_ema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Puff-Episode vs. EOD EMA inconsistency percentage by entries is: ', 0.32727272727272727)\n",
      "('Puff-Episode vs. EOD EMA inconsistency percentage by day is: ', 0.12727272727272726)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "episode_vs_eod(puff_episode_alt, eod_ema_alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Puff-Episode vs. EOD EMA inconsistency percentage by entries is: ', 0.2939068100358423)\n",
      "('Puff-Episode vs. EOD EMA inconsistency percentage by day is: ', 0.12903225806451613)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "episode_vs_eod(puff_episode_backup, eod_ema_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
