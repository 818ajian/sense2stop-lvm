{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as datetime\n",
    "import os\n",
    "os.getcwd()\n",
    "dir = \"/Users/walterdempsey/Box/MD2K Processed Data/smoking-lvm-cleaned-data/\"\n",
    "os.chdir(dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different hour windows in eod_ema\n",
    "keys = ['8to9', '9to10', '10to11', '11to12','12to13','13to14','14to15','15to16','16to17','17to18','18to19','19to20']\n",
    "\n",
    "random_accptresponse = ['1 - 19 Minutes', '20 - 39 Minutes', '40 - 59 Minutes', \n",
    "                    '60 - 79 Minutes', '80 - 100 Minutes']\n",
    "random_dictionary = {'1 - 19 Minutes': 10, \n",
    "                     '20 - 39 Minutes': 30, \n",
    "                     '40 - 59 Minutes':50,\n",
    "                     '60 - 79 Minutes':70, \n",
    "                     '80 - 100 Minutes':90 } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "random_original_cloud_ids = [201, 203, 206, 210, 221, 226, 229] \n",
    "eod_original_cloud_ids = [201, 203, 206, 221, 229] \n",
    "\n",
    "random_ema = pd.read_csv(dir + 'random-ema.csv')\n",
    "random_ema = random_ema.drop(['offset'], axis = 1)\n",
    "eod_ema = pd.read_csv(dir + 'eod-ema.csv')\n",
    "eod_ema = eod_ema.drop(['offset'], axis = 1)\n",
    "\n",
    "random_ema_alternative = pd.read_csv(dir + 'random-ema-alternative.csv')\n",
    "eod_ema_alternative = pd.read_csv(dir + 'eod-ema-alternative.csv')\n",
    "\n",
    "random_ema_backup = pd.read_csv(dir + 'random-ema-backup.csv')\n",
    "eod_ema_backup = pd.read_csv(dir + 'eod-ema-backup.csv')\n",
    "\n",
    "temp_random_original = random_ema[random_ema['participant_id'].isin(random_original_cloud_ids)]\n",
    "temp_random_alt = random_ema_alternative[~random_ema_alternative['participant_id'].isin(random_original_cloud_ids)]\n",
    "\n",
    "random_complete = pd.concat([temp_random_original, temp_random_alt, random_ema_backup])\n",
    "\n",
    "temp_eod_original = eod_ema[eod_ema['participant_id'].isin(eod_original_cloud_ids)]\n",
    "temp_eod_alt = eod_ema_alternative[~eod_ema_alternative['participant_id'].isin(eod_original_cloud_ids)]\n",
    "\n",
    "eod_complete = pd.concat([temp_eod_original, temp_eod_alt, eod_ema_backup])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{201: set(),\n",
       " 202: {(2017, 6, 26, 15), (2017, 7, 6, 15)},\n",
       " 203: set(),\n",
       " 204: set(),\n",
       " 205: {(2017, 8, 29, 16)},\n",
       " 206: set(),\n",
       " 207: {(2017, 9, 18, 17)},\n",
       " 208: {(2017, 9, 19, 10), (2017, 9, 22, 8), (2017, 9, 22, 16)},\n",
       " 209: {(2017, 10, 1, 12), (2017, 10, 1, 17)},\n",
       " 210: {(2017, 9, 29, 12)},\n",
       " 211: {(2017, 10, 3, 9), (2017, 10, 13, 16), (2017, 10, 16, 9)},\n",
       " 212: set(),\n",
       " 213: set(),\n",
       " 214: {(2017, 10, 23, 14),\n",
       "  (2017, 10, 24, 12),\n",
       "  (2017, 10, 24, 17),\n",
       "  (2017, 10, 25, 17),\n",
       "  (2017, 10, 30, 10)},\n",
       " 215: {(2017, 10, 27, 9)},\n",
       " 216: {(2017, 11, 6, 19), (2017, 11, 7, 18), (2017, 11, 9, 17)},\n",
       " 217: {(2017, 10, 31, 9),\n",
       "  (2017, 10, 31, 19),\n",
       "  (2017, 11, 1, 10),\n",
       "  (2017, 11, 1, 17),\n",
       "  (2017, 11, 2, 11),\n",
       "  (2017, 11, 3, 9),\n",
       "  (2017, 11, 5, 14),\n",
       "  (2017, 11, 8, 16),\n",
       "  (2017, 11, 10, 13)},\n",
       " 218: {(2017, 11, 3, 17),\n",
       "  (2017, 11, 4, 9),\n",
       "  (2017, 11, 4, 16),\n",
       "  (2017, 11, 5, 9),\n",
       "  (2017, 11, 5, 18),\n",
       "  (2017, 11, 6, 9),\n",
       "  (2017, 11, 6, 16),\n",
       "  (2017, 11, 7, 9)},\n",
       " 219: {(2017, 11, 16, 17),\n",
       "  (2017, 11, 17, 15),\n",
       "  (2017, 11, 17, 18),\n",
       "  (2017, 11, 19, 18),\n",
       "  (2017, 11, 21, 18),\n",
       "  (2017, 11, 22, 10),\n",
       "  (2017, 11, 22, 18),\n",
       "  (2017, 11, 23, 19),\n",
       "  (2017, 11, 24, 17),\n",
       "  (2017, 11, 26, 9),\n",
       "  (2017, 11, 26, 18)},\n",
       " 220: {(2017, 11, 28, 9),\n",
       "  (2017, 11, 28, 13),\n",
       "  (2017, 11, 29, 9),\n",
       "  (2017, 11, 29, 14),\n",
       "  (2017, 11, 29, 17),\n",
       "  (2017, 11, 30, 8)},\n",
       " 221: {(2017, 11, 27, 14),\n",
       "  (2017, 11, 28, 17),\n",
       "  (2017, 11, 29, 18),\n",
       "  (2017, 11, 30, 9)},\n",
       " 222: {(2017, 12, 4, 14),\n",
       "  (2017, 12, 5, 11),\n",
       "  (2017, 12, 7, 10),\n",
       "  (2017, 12, 7, 14),\n",
       "  (2017, 12, 7, 17),\n",
       "  (2017, 12, 8, 12)},\n",
       " 223: set(),\n",
       " 224: {(2018, 1, 22, 11),\n",
       "  (2018, 1, 23, 10),\n",
       "  (2018, 1, 23, 14),\n",
       "  (2018, 1, 23, 18),\n",
       "  (2018, 1, 24, 11),\n",
       "  (2018, 1, 24, 15),\n",
       "  (2018, 1, 25, 11),\n",
       "  (2018, 1, 25, 13),\n",
       "  (2018, 1, 25, 16),\n",
       "  (2018, 1, 26, 14),\n",
       "  (2018, 1, 26, 17),\n",
       "  (2018, 1, 27, 13),\n",
       "  (2018, 1, 31, 14),\n",
       "  (2018, 2, 1, 12),\n",
       "  (2018, 2, 1, 17),\n",
       "  (2018, 2, 2, 9),\n",
       "  (2018, 2, 3, 12)},\n",
       " 225: {(2018, 1, 29, 11),\n",
       "  (2018, 1, 29, 12),\n",
       "  (2018, 1, 30, 11),\n",
       "  (2018, 1, 31, 9),\n",
       "  (2018, 1, 31, 17),\n",
       "  (2018, 2, 2, 16),\n",
       "  (2018, 2, 2, 18),\n",
       "  (2018, 2, 3, 11),\n",
       "  (2018, 2, 3, 15),\n",
       "  (2018, 2, 4, 11),\n",
       "  (2018, 2, 4, 14),\n",
       "  (2018, 2, 6, 10),\n",
       "  (2018, 2, 7, 17),\n",
       "  (2018, 2, 8, 14)},\n",
       " 227: {(2018, 2, 10, 13),\n",
       "  (2018, 2, 10, 17),\n",
       "  (2018, 2, 11, 11),\n",
       "  (2018, 2, 12, 16),\n",
       "  (2018, 2, 12, 18),\n",
       "  (2018, 2, 13, 11),\n",
       "  (2018, 2, 13, 15),\n",
       "  (2018, 2, 13, 18),\n",
       "  (2018, 2, 13, 19),\n",
       "  (2018, 2, 14, 10),\n",
       "  (2018, 2, 14, 14),\n",
       "  (2018, 2, 14, 17),\n",
       "  (2018, 2, 15, 11),\n",
       "  (2018, 2, 15, 14),\n",
       "  (2018, 2, 15, 16),\n",
       "  (2018, 2, 15, 17),\n",
       "  (2018, 2, 15, 18),\n",
       "  (2018, 2, 16, 10),\n",
       "  (2018, 2, 16, 14),\n",
       "  (2018, 2, 16, 19),\n",
       "  (2018, 2, 17, 10),\n",
       "  (2018, 2, 17, 15),\n",
       "  (2018, 2, 17, 18),\n",
       "  (2018, 2, 18, 10),\n",
       "  (2018, 2, 18, 13),\n",
       "  (2018, 2, 18, 17),\n",
       "  (2018, 2, 19, 10),\n",
       "  (2018, 2, 19, 13),\n",
       "  (2018, 2, 20, 8),\n",
       "  (2018, 2, 20, 18),\n",
       "  (2018, 2, 22, 14)},\n",
       " 228: {(2018, 4, 7, 14), (2018, 4, 17, 15)},\n",
       " 229: {(2018, 4, 14, 9),\n",
       "  (2018, 4, 16, 17),\n",
       "  (2018, 4, 18, 13),\n",
       "  (2018, 4, 18, 16),\n",
       "  (2018, 4, 19, 14)},\n",
       " 230: {(2018, 6, 18, 18),\n",
       "  (2018, 6, 19, 11),\n",
       "  (2018, 6, 19, 14),\n",
       "  (2018, 6, 20, 15)},\n",
       " 231: {(2018, 6, 18, 19),\n",
       "  (2018, 6, 19, 17),\n",
       "  (2018, 6, 20, 15),\n",
       "  (2018, 6, 21, 17),\n",
       "  (2018, 6, 23, 15)},\n",
       " 232: set(),\n",
       " 233: {(2018, 7, 14, 15), (2018, 7, 20, 18), (2018, 7, 24, 14)},\n",
       " 234: {(2018, 7, 17, 13), (2018, 7, 18, 11), (2018, 7, 18, 19)},\n",
       " 235: {(2018, 7, 20, 16), (2018, 7, 21, 10)},\n",
       " 236: set(),\n",
       " 237: set()}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a list of all contingent event-times between 8AM and 8PM\n",
    "# Throw away observations for 'when_smoke' is nan or \n",
    "# 'More than 30 minutes' to ensure we can calculate a meaningful \n",
    "# quantity.\n",
    "days_smoked = {}\n",
    "for index, row in random_complete.iterrows():\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    if row['when_smoke'] in random_accptresponse:\n",
    "        time = time - datetime.timedelta(minutes=random_dictionary[row['when_smoke']])\n",
    "    date = (time.year, time.month, time.day, time.hour)\n",
    "    if row['participant_id'] not in days_smoked:\n",
    "        days_smoked[row['participant_id']] = set()\n",
    "    if 8 <= date[3] < 20 and row['when_smoke'] in random_accptresponse:        \n",
    "        days_smoked[row['participant_id']].add(date)\n",
    "\n",
    "days_smoked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [201, 2017, 6, ..., 0.0, 0.0, 0.0],\n",
       "       [203, 2017, 8, ..., 0.0, 0.0, 0.0],\n",
       "       ...,\n",
       "       [236, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0],\n",
       "       [237, 2018, 7, ..., 0.0, 0.0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a list of id + dates for eod\n",
    "# Use to look \n",
    "eod_dates = []\n",
    "for irow in range(0,eod_complete.shape[0]):\n",
    "    row = eod_complete.iloc[irow]\n",
    "    if row['status'] == \"MISSED\":\n",
    "        continue\n",
    "    try:\n",
    "        time = datetime.datetime.strptime(row['date'], '%m/%d/%Y %H:%M')\n",
    "    except:\n",
    "        time = datetime.datetime.strptime(row['date'], '%Y-%m-%d %H:%M:%S')\n",
    "    if time.hour  == 0 or time.hour == 1:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day-1])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    else:\n",
    "        date = np.array([row['participant_id'], time.year, time.month, time.day])\n",
    "        date = np.append(date, np.array(row[keys]))\n",
    "    eod_dates.append(date)\n",
    "    \n",
    "eod_dates = np.asarray(eod_dates)\n",
    "eod_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current hour only:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.4\n",
      "Mean of Fraction agreement across indidivuals: 0.434\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.364\n",
      "\n",
      "Plus-minus one hour:\n",
      "Aggregated data, Fraction agreement between EC and EOD: 0.675\n",
      "Mean of Fraction agreement across indidivuals: 0.703\n",
      "Standard deviation of Fraction agreement across indidivuals: 0.34\n"
     ]
    }
   ],
   "source": [
    "# For participants with both EC and EOD measurements,\n",
    "# on days when you give both, we ask whether they agree,\n",
    "# up to the current hour, or +- 1 hour in either direction.\n",
    "# The +-1 is max/min by 8AM and 8PM respectively.\n",
    "matching_counts = []\n",
    "max_iloc = 15; min_iloc = 4\n",
    "for id in set(days_smoked.keys()) & set(eod_dates[:,0]):\n",
    "    eod_dates_id = np.where(eod_dates[:,0] == id) \n",
    "    eod_dates_subset = eod_dates[eod_dates_id[0],:]\n",
    "    total_count_id = 0\n",
    "    hour_count_id_true = 0\n",
    "    twohour_count_id_true = 0\n",
    "    if days_smoked[id] == set():\n",
    "        continue\n",
    "    for ec_time in days_smoked[id]:\n",
    "        row_iloc = np.where((eod_dates_subset[:,1:4] == ec_time[0:3]).all(axis=1))[0]\n",
    "        if not row_iloc.size > 0:\n",
    "            continue\n",
    "        total_count_id+=1\n",
    "        row = eod_dates_subset[row_iloc][0]\n",
    "        ec_iloc = range(8,20).index(ec_time[3])+4\n",
    "        if row[ec_iloc]==1:\n",
    "            hour_count_id_true+=1\n",
    "        if any(row[range(max(min_iloc, ec_iloc-1), min(max_iloc, ec_iloc+1)+1)] == 1):\n",
    "            twohour_count_id_true+=1\n",
    "    matching_counts.append(np.array([total_count_id, hour_count_id_true, twohour_count_id_true], dtype='f'))\n",
    "\n",
    "matching_counts = np.asarray(matching_counts)\n",
    "\n",
    "matching_counts = np.delete(matching_counts, (np.where(matching_counts[:,0] == 0)[0][0]), axis=0)\n",
    "\n",
    "fraction_per_id_onehour = np.divide(matching_counts[:,1],matching_counts[:,0])\n",
    "fraction_per_id_twohour = np.divide(matching_counts[:,2],matching_counts[:,0])\n",
    "\n",
    "aggregate_matching_counts = np.sum(matching_counts, axis=0)\n",
    "\n",
    "aggregate_frac_onehour = aggregate_matching_counts[1]/aggregate_matching_counts[0]\n",
    "aggregate_frac_twohour = aggregate_matching_counts[2]/aggregate_matching_counts[0]\n",
    "\n",
    "print 'Current hour only:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_onehour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_onehour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_onehour),3))\n",
    "print\n",
    "print 'Plus-minus one hour:'\n",
    "print 'Aggregated data, Fraction agreement between EC and EOD: %s' % (np.round(aggregate_frac_twohour,3))\n",
    "print 'Mean of Fraction agreement across indidivuals: %s' % (np.round(np.mean(fraction_per_id_twohour),3))\n",
    "print 'Standard deviation of Fraction agreement across indidivuals: %s' %  (np.round(np.std(fraction_per_id_twohour),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA p-value for current hour: 5.59951406752246e-05\n",
      "ANOVA p-value for plus-minus one hour: 5.938678826478139e-07\n"
     ]
    }
   ],
   "source": [
    "# Compute an anova decomposition using the bernoulli likelihood\n",
    "# This will test if there are significant differences across\n",
    "# individuals.\n",
    "\n",
    "llik_onehour = 0; llik_twohour = 0\n",
    "for i in range(0, fraction_per_id_onehour.size):\n",
    "    num_ones_onehour = matching_counts[i,1]\n",
    "    num_zeros_onehour = matching_counts[i,0] - matching_counts[i,1]\n",
    "    if num_ones_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_ones_onehour, np.log(fraction_per_id_onehour[i]))\n",
    "    if num_zeros_onehour > 0.0:\n",
    "        llik_onehour += np.multiply(num_zeros_onehour, np.log(1-fraction_per_id_onehour[i]))\n",
    "    num_ones_twohour = matching_counts[i,2]\n",
    "    num_zeros_twohour = matching_counts[i,0] - matching_counts[i,2]\n",
    "    if num_ones_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_ones_twohour, np.log(fraction_per_id_twohour[i]))\n",
    "    if num_zeros_twohour > 0.0:\n",
    "        llik_twohour += np.multiply(num_zeros_twohour, np.log(1-fraction_per_id_twohour[i]))\n",
    "\n",
    "agg_num_ones = aggregate_matching_counts[1]\n",
    "agg_num_zeros = aggregate_matching_counts[0] - aggregate_matching_counts[1]\n",
    "agg_llik_onehour = agg_num_ones*np.log(aggregate_frac_onehour)+agg_num_zeros*np.log(1-aggregate_frac_onehour)\n",
    "\n",
    "D_onehour = -2*agg_llik_onehour + 2*llik_onehour\n",
    "\n",
    "agg_num_ones_twohour = aggregate_matching_counts[2]\n",
    "agg_num_zeros_twohour = aggregate_matching_counts[0] - aggregate_matching_counts[2]\n",
    "agg_llik_twohour = agg_num_ones_twohour*np.log(aggregate_frac_twohour)+agg_num_zeros_twohour*np.log(1-aggregate_frac_twohour)\n",
    "\n",
    "D_twohour = -2*agg_llik_twohour + 2*llik_twohour\n",
    "\n",
    "from scipy.stats import chi2\n",
    "n = aggregate_matching_counts[0]\n",
    "k = matching_counts.shape[0]\n",
    "df = k-1\n",
    "\n",
    "print 'ANOVA p-value for current hour: %s' % (1-chi2.cdf(D_onehour, df))\n",
    "print 'ANOVA p-value for plus-minus one hour: %s' % (1-chi2.cdf(D_twohour, df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
